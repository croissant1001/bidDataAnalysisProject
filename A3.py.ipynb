{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = \"/Users/liyitong/Desktop/BigData/checkpoint\"\n",
    "path2 = \"/Users/liyitong/Desktop/BigData/dataset\"\n",
    "main = \"/Users/liyitong/Desktop/BigData/spark/notebooks/\"\n",
    "path3 = \"/Users/liyitong/Desktop/BigData/checkpoint_rf\"\n",
    "path4 = \"/Users/liyitong/Desktop/BigData/dataset_rf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.52:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.52:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff69c23ac50>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 12:27:04 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "socketDF = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 8080).load()\n",
    "socketDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 12:27:16 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = socketDF \\\n",
    "        .writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .format(\"json\")\\\n",
    "        .option('checkpointLocation',path1)\\\n",
    "        .option('path',path2)\\\n",
    "        .start()\n",
    "query.awaitTermination(timeout=600)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.json(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('datetime', StringType(), True),\n",
    "        StructField('channel', StringType(), True),\n",
    "        StructField('username', StringType(), True),\n",
    "        StructField('message', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = dataset.withColumn(\"value\", from_json(\"value\", schema))\\\n",
    "    .select(col('value.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1749\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset.columns), dataset.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------------+--------------------+\n",
      "|            datetime|     channel|         username|             message|\n",
      "+--------------------+------------+-----------------+--------------------+\n",
      "|2022-05-20T23:34:...|#cohhcarnage|fluorescentpickle|@georgreaver This...|\n",
      "|2022-05-20T23:34:...|     #quin69|       punishermk|           PainChamp|\n",
      "|2022-05-20T23:34:...|#cohhcarnage|      n1njac00kie|is there a perma ...|\n",
      "|2022-05-20T23:34:...|     #quin69|           textue|that guy is a ret...|\n",
      "|2022-05-20T23:34:...|     #quin69|     friendlyfyre|WhomstInquired Wh...|\n",
      "|2022-05-20T23:34:...|     #quin69|        lojackatk|               ICANT|\n",
      "|2022-05-20T23:34:...|     #quin69|            affx5|triggered libtard...|\n",
      "|2022-05-20T23:34:...|     #quin69|         tyree372|he loves to eat b...|\n",
      "|2022-05-20T23:34:...|     #quin69|    demonstrate86|I work at Microso...|\n",
      "|2022-05-20T23:34:...|     #quin69|          omgacow|That guy is proof...|\n",
      "|2022-05-20T23:34:...|     #quin69|           adzzzu| @GiCi HOLY GIGACHAD|\n",
      "|2022-05-20T23:34:...|     #quin69|        dubba_225|he is a professor...|\n",
      "|2022-05-20T23:34:...|     #quin69|      tastygravel|         hes a clown|\n",
      "|2022-05-20T23:34:...|     #quin69|            mrizu|@GiCi what u grad...|\n",
      "|2022-05-20T23:34:...|     #quin69|          alanrld|              copium|\n",
      "|2022-05-20T23:34:...|     #quin69|        biscy_311|       WH OMEGALUL â“|\n",
      "|2022-05-20T23:34:...|     #quin69|           speyll|a scientist with ...|\n",
      "|2022-05-20T23:34:...|     #quin69|           carvis|retards in chat LOLW|\n",
      "|2022-05-20T23:34:...|     #quin69|    sweaty_freddy|         JP DIESOFDD|\n",
      "|2022-05-20T23:34:...|#cohhcarnage|        evolvertv|I am sorry I just...|\n",
      "+--------------------+------------+-----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = dataset.randomSplit([0.7, 0.3], seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import StandardScaler,FeatureHasher\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline.\n",
    "indexer = StringIndexer(inputCol=\"channel\", outputCol=\"label\")\n",
    "hasher = FeatureHasher(inputCols=[\"message\", \"username\"],\n",
    "                       outputCol=\"indexfeatures\")\n",
    "scaler = StandardScaler(inputCol=\"indexfeatures\", outputCol=\"features\",\n",
    "                        withStd=True, withMean=True)\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "pipeline = Pipeline(stages=[indexer, hasher, scaler, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(hasher.numFeatures, [10, 100, 1000]) \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "lrcvModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "lrcvPath = main + \"lrcvModel\"\n",
    "lrcvModel.bestModel.write().overwrite().save(lrcvPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"channel\", outputCol=\"label\")\n",
    "hasher = FeatureHasher(inputCols=[\"message\", \"username\"],\n",
    "                       outputCol=\"features\")\n",
    "nb = NaiveBayes(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer, hasher, nb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "nbparamGrid = ParamGridBuilder()\\\n",
    "               .addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\\\n",
    "               .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 3-fold CrossValidator\n",
    "nbcv = CrossValidator(estimator = pipeline,\n",
    "                    estimatorParamMaps = nbparamGrid,\n",
    "                    evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 19:53:02 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:06 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:09 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:12 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:15 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:18 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:22 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:25 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:27 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:30 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:32 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:34 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:37 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:40 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:42 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:44 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:46 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "22/05/26 19:53:48 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nbcvModel = nbcv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 19:55:52 WARN TaskSetManager: Stage 170 contains a task of very large size (4185 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "nbcvPath = main + \"nbcvModel\"\n",
    "nbcvModel.bestModel.write().overwrite().save(nbcvPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machines ---- Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = StringIndexer(inputCol=\"channel\", outputCol=\"label\")\n",
    "hasher = FeatureHasher(inputCols=[\"message\", \"username\"],\n",
    "                       outputCol=\"features\")\n",
    "lsvc = LinearSVC(maxIter=10, labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer, hasher, lsvc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "               .addGrid(lsvc.regParam, [0.1, 0.01])\\\n",
    "               .build()\n",
    "cvlsvc = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)  # use 3+ folds in practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:26:39 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:26:49 WARN BlockManager: Asked to remove block broadcast_2620, which does not exist\n",
      "22/05/21 00:27:01 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:27:27 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:27:45 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:28:13 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:28:33 WARN BlockManager: Asked to remove block broadcast_3125, which does not exist\n",
      "22/05/21 00:28:33 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "22/05/21 00:28:42 WARN BlockManager: Asked to remove block broadcast_3161, which does not exist\n"
     ]
    }
   ],
   "source": [
    "# Run cross-validation, and choose the best set of parameters.\n",
    "cvlsvcModel = cvlsvc.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:29:33 WARN TaskSetManager: Stage 2761 contains a task of very large size (2094 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "# Save the best model\n",
    "cvlsvcPath = main + \"cvlsvcModel\"\n",
    "cvlsvcModel.bestModel.write().overwrite().save(cvlsvcPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import PipelineModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pickled model via pipeline api\n",
    "lrcv = PipelineModel.load(lrcvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrcv.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9198:===================================>                  (31 + 4) / 47]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.8635153990552861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 9198:==================================================>   (44 + 3) / 47]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "Accuracy =  evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\" + str(Accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "nbcv = PipelineModel.load(nbcvPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbpredictions = nbcv.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 19:56:16 WARN DAGScheduler: Broadcasting large task binary with size 4.1 MiB\n",
      "[Stage 185:==================================>                    (24 + 4) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8300804785677398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 185:==============================================>        (32 + 4) / 38]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# cvModel uses the best model found from the Cross Validation\n",
    "# Evaluate best model\n",
    "nbevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy:', nbevaluator.evaluate(nbpredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvlsvc = PipelineModel.load(cvlsvcPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:30:38 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "[Stage 2810:===========================================>          (60 + 4) / 74]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9378240217656277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 2810:====================================================> (72 + 2) / 74]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cvlsvcpredictions = cvlsvc.transform(test)\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy:', evaluator.evaluate(cvlsvcpredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used a small dataset to estimate random forest model since the memory space is not enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 12:29:31 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "socketDF = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 8080).load()\n",
    "socketDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/27 12:29:38 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "query = socketDF \\\n",
    "        .writeStream\\\n",
    "        .outputMode(\"append\")\\\n",
    "        .format(\"json\")\\\n",
    "        .option('checkpointLocation',path3)\\\n",
    "        .option('path',path4)\\\n",
    "        .start()\n",
    "query.awaitTermination(timeout=300)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField('datetime', StringType(), True),\n",
    "        StructField('channel', StringType(), True),\n",
    "        StructField('username', StringType(), True),\n",
    "        StructField('message', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "df = df.withColumn(\"value\", from_json(\"value\", schema))\\\n",
    "    .select(col('value.*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 4:===========================================>             (29 + 4) / 38]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(len(df.columns), df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train, rf_test= df.randomSplit([0.5, 0.5], seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "856"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure an ML pipeline.\n",
    "indexer = StringIndexer(inputCol=\"channel\", outputCol=\"label\")\n",
    "#indexer = StringIndexer(inputCol=\"channel\", outputCol=\"label\")\n",
    "hasher = FeatureHasher(inputCols=[\"message\", \"username\"],\n",
    "                       outputCol=\"features\")\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "pipeline = Pipeline(stages=[indexer, hasher, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/26 19:40:30 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "22/05/26 19:40:36 WARN DAGScheduler: Broadcasting large task binary with size 1034.7 KiB\n",
      "22/05/26 19:40:37 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/26 19:40:41 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/26 19:40:44 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/26 19:40:46 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "22/05/26 19:40:48 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train model with Training Data\n",
    "rfModel = pipeline.fit(rf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "rfPath = main + \"rfModel\"\n",
    "rfModel.write().overwrite().save(rfPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 45:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5711333579404357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 57:================================================>       (33 + 4) / 38]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rf = PipelineModel.load(rfPath)\n",
    "rfpredictions = rf.transform(rf_test)\n",
    "rfevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "print('Accuracy:', rfevaluator.evaluate(rfpredictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployed Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf, struct, array, col, lit\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(lrcvPath)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"channel\",\"datetime\",\"message\",\"username\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:31:28 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:28 WARN BlockManager: Block input-0-1653085888200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:31:29 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:29 WARN BlockManager: Block input-0-1653085889200 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:31:30 =========\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "|channel|            datetime|             message|      username|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM ratJAM|     elfatbear|\n",
      "| #lirik|2022-05-20T22:31:...|NomuJem xar2EDM  ...|        aur1so|\n",
      "| #lirik|2022-05-20T22:31:...|         pepeFASTJAM|       mahdewd|\n",
      "| #lirik|2022-05-20T22:31:...|song is: utopia b...|superbirdy2037|\n",
      "|#quin69|2022-05-20T22:31:...|\u0001ACTION Join the ...|streamelements|\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|       xaolinn|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:31:30 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:30 WARN BlockManager: Block input-0-1653085890200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:31:31 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:31 WARN BlockManager: Block input-0-1653085891200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "|channel|            datetime|             message|      username|label|prediction|\n",
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM ratJAM|     elfatbear|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|NomuJem xar2EDM  ...|        aur1so|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|         pepeFASTJAM|       mahdewd|  0.0|       1.0|\n",
      "| #lirik|2022-05-20T22:31:...|song is: utopia b...|superbirdy2037|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:31:...|\u0001ACTION Join the ...|streamelements|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|       xaolinn|  0.0|       1.0|\n",
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:31:32 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:32 WARN BlockManager: Block input-0-1653085892200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:31:33 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:33 WARN BlockManager: Block input-0-1653085893200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:31:34 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:34 WARN BlockManager: Block input-0-1653085894200 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:31:35 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:31:35 WARN BlockManager: Block input-0-1653085895200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:31:35 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:31:35 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/05/21 00:31:35 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:31:35 WARN ReceiverSupervisorImpl: Receiver has been stopped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:31:40 =========\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "|channel|            datetime|             message|        username|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|          xxtayo|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|        nuilzero|\n",
      "| #lirik|2022-05-20T22:31:...|                LGen|          ninput|\n",
      "| #lirik|2022-05-20T22:31:...|             CatsJAM|        rotator1|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|         sikefox|\n",
      "| #lirik|2022-05-20T22:31:...|         MILKMANRAVE|  originaljacket|\n",
      "| #lirik|2022-05-20T22:31:...|         MILKMANRAVE|          crioos|\n",
      "| #lirik|2022-05-20T22:31:...|djFR xar2EDM FRpl...|        mrbeannm|\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|           lctr_|\n",
      "| #lirik|2022-05-20T22:31:...|           rabbitJAM|       kocheng11|\n",
      "| #lirik|2022-05-20T22:31:...|pepeD pepeD pepeD...|       mistletea|\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM rat...|     guyduude420|\n",
      "| #lirik|2022-05-20T22:31:...|xar2EDM docPls xa...|          scluse|\n",
      "|#quin69|2022-05-20T22:31:...|  get onsaught ring?|verygoodplayerow|\n",
      "| #lirik|2022-05-20T22:31:...|RareNayna RareNom...|      marcuswolf|\n",
      "|#quin69|2022-05-20T22:31:...| onslought probs bis|       saltyvest|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|        nuilzero|\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM ratJAM|       alejan2ro|\n",
      "| #lirik|2022-05-20T22:31:...|       ratJAM ratJAM|       elfatbear|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "|channel|            datetime|             message|        username|label|prediction|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|          xxtayo|  0.0|       1.0|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|        nuilzero|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|                LGen|          ninput|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|             CatsJAM|        rotator1|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|         sikefox|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|         MILKMANRAVE|  originaljacket|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|         MILKMANRAVE|          crioos|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|djFR xar2EDM FRpl...|        mrbeannm|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|              docPls|           lctr_|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|           rabbitJAM|       kocheng11|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|pepeD pepeD pepeD...|       mistletea|  0.0|       1.0|\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM rat...|     guyduude420|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|xar2EDM docPls xa...|          scluse|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:31:...|  get onsaught ring?|verygoodplayerow|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:31:...|RareNayna RareNom...|      marcuswolf|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:31:...| onslought probs bis|       saltyvest|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|              catJAM|        nuilzero|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|ratJAM ratJAM ratJAM|       alejan2ro|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:31:...|       ratJAM ratJAM|       elfatbear|  0.0|       1.0|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(nbcvPath)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"channel\",\"datetime\",\"message\",\"username\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:32:10 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:10 WARN BlockManager: Block input-0-1653085930400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:11 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:11 WARN BlockManager: Block input-0-1653085931400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:12 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:12 WARN BlockManager: Block input-0-1653085932400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:13 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:13 WARN BlockManager: Block input-0-1653085933400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:14 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:14 WARN BlockManager: Block input-0-1653085934400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:15 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:15 WARN BlockManager: Block input-0-1653085935600 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:16 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:16 WARN BlockManager: Block input-0-1653085936400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:18 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:18 WARN BlockManager: Block input-0-1653085938400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:19 WARN BlockManager: Block input-0-1653085939400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:19 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:19 WARN BlockManager: Block input-0-1653085939600 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:32:20 =========\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "|channel|            datetime|             message|        username|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "|#quin69|2022-05-20T22:32:...| 3mil is only ignite|        domdomuh|\n",
      "| #lirik|2022-05-20T22:32:...|              jumpFR|       alejan2ro|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls VI...|          scluse|\n",
      "|#quin69|2022-05-20T22:32:...|     it will be less|verygoodplayerow|\n",
      "| #lirik|2022-05-20T22:32:...|FRpls2 FRpls Jamm...|      marcuswolf|\n",
      "| #lirik|2022-05-20T22:32:...|               song?|          daz__7|\n",
      "|#quin69|2022-05-20T22:32:...|i wish i could mu...|    dunkeysballs|\n",
      "| #lirik|2022-05-20T22:32:...|            pandaPls|          moasat|\n",
      "| #lirik|2022-05-20T22:32:...|             neonPls|        nuilzero|\n",
      "| #lirik|2022-05-20T22:32:...|I need some heali...|           nrnee|\n",
      "|#quin69|2022-05-20T22:32:...|       Meta cuck YEP|          z_neex|\n",
      "| #lirik|2022-05-20T22:32:...|     !vote Golf Gang|         sikefox|\n",
      "| #lirik|2022-05-20T22:32:...|              bonkFR|     walker___tv|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls SPEED|          scluse|\n",
      "| #lirik|2022-05-20T22:32:...|             xar2EDM|        wruktarr|\n",
      "|#quin69|2022-05-20T22:32:...|                !RIP| h4l_ghostlegion|\n",
      "|#quin69|2022-05-20T22:32:...|https://youtu.be/...|  streamelements|\n",
      "| #lirik|2022-05-20T22:32:...|Bro do you guys e...|       eriqtendo|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:32:20 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:20 WARN BlockManager: Block input-0-1653085940400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:21 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/05/21 00:32:21 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/05/21 00:32:21 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "|channel|            datetime|             message|        username|label|prediction|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "|#quin69|2022-05-20T22:32:...| 3mil is only ignite|        domdomuh|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|              jumpFR|       alejan2ro|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls VI...|          scluse|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|     it will be less|verygoodplayerow|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:32:...|FRpls2 FRpls Jamm...|      marcuswolf|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|               song?|          daz__7|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|i wish i could mu...|    dunkeysballs|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|            pandaPls|          moasat|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|             neonPls|        nuilzero|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|I need some heali...|           nrnee|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|       Meta cuck YEP|          z_neex|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|     !vote Golf Gang|         sikefox|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|              bonkFR|     walker___tv|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls SPEED|          scluse|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|             xar2EDM|        wruktarr|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|                !RIP| h4l_ghostlegion|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:32:...|https://youtu.be/...|  streamelements|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:32:...|Bro do you guys e...|       eriqtendo|  0.0|       0.0|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:32:21 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:21 WARN BlockManager: Block input-0-1653085941400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:22 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:22 WARN BlockManager: Block input-0-1653085942400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:32:24 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:32:24 WARN BlockManager: Block input-0-1653085944600 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:32:25 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:32:25 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/05/21 00:32:25 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:32:25 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:32:30 =========\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "|channel|            datetime|             message|      username|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "| #lirik|2022-05-20T22:32:...|             xar2EDM|   dinosaure56|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls AD...|        scluse|\n",
      "| #lirik|2022-05-20T22:32:...|LPPM xar2EDM LPPM...|       sikefox|\n",
      "|#quin69|2022-05-20T22:32:...|   goosePls EDM HOLY| vanitylicious|\n",
      "|#quin69|2022-05-20T22:32:...|                !rip|      veyristv|\n",
      "|#quin69|2022-05-20T22:32:...|https://youtu.be/...|streamelements|\n",
      "| #lirik|2022-05-20T22:32:...|              ratJAM|        inkode|\n",
      "|#quin69|2022-05-20T22:32:...|you really overes...|       zofurie|\n",
      "+-------+--------------------+--------------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:32:30 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n",
      "22/05/21 00:32:30 WARN DAGScheduler: Broadcasting large task binary with size 4.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "|channel|            datetime|             message|      username|label|prediction|\n",
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "| #lirik|2022-05-20T22:32:...|             xar2EDM|   dinosaure56|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|xar2EDM docPls AD...|        scluse|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:32:...|LPPM xar2EDM LPPM...|       sikefox|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|   goosePls EDM HOLY| vanitylicious|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:32:...|                !rip|      veyristv|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:32:...|https://youtu.be/...|streamelements|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:32:...|              ratJAM|        inkode|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:32:...|you really overes...|       zofurie|  1.0|       0.0|\n",
      "+-------+--------------------+--------------------+--------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(cvlsvcPath)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"channel\",\"datetime\",\"message\",\"username\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:33:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:40 WARN BlockManager: Block input-0-1653086020000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:40 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:40 WARN BlockManager: Block input-0-1653086020200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:41 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:41 WARN BlockManager: Block input-0-1653086021200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:42 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:42 WARN BlockManager: Block input-0-1653086022000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:43 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:43 WARN BlockManager: Block input-0-1653086023000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:45 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:45 WARN BlockManager: Block input-0-1653086025200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:46 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:46 WARN BlockManager: Block input-0-1653086026200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:47 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:47 WARN BlockManager: Block input-0-1653086027000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:49 WARN BlockManager: Block input-0-1653086029000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:49 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:49 WARN BlockManager: Block input-0-1653086029200 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:33:50 =========\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|channel|            datetime|             message|            username|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|#quin69|2022-05-20T22:33:...|                  70|               raduk|\n",
      "|#quin69|2022-05-20T22:33:...|       infernal cry?|          airfusionz|\n",
      "| #lirik|2022-05-20T22:33:...|               24hr?|       lootumshootum|\n",
      "|#quin69|2022-05-20T22:33:...|    level ur EA LOLW|             catatrr|\n",
      "|#quin69|2022-05-20T22:33:...|                  72|          deathnell3|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|            mrbeannm|\n",
      "|#quin69|2022-05-20T22:33:...|HIDEOUT SIMULATOR...|       culliganman76|\n",
      "|#quin69|2022-05-20T22:33:...|             Weirdga|          adamoftime|\n",
      "|#quin69|2022-05-20T22:33:...|                THIS|      chrono_crosser|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|           alejan2ro|\n",
      "|#quin69|2022-05-20T22:33:...|                  69|   badly_coded_brain|\n",
      "| #lirik|2022-05-20T22:33:...|     PauseChamp hodl|            mrbeannm|\n",
      "|#quin69|2022-05-20T22:33:...|   THIS one is maxed|mikeauxsmoll_____...|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:33:50 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:50 WARN BlockManager: Block input-0-1653086030200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:51 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:51 WARN BlockManager: Block input-0-1653086031000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:51 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/05/21 00:33:51 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/05/21 00:33:51 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|channel|            datetime|             message|            username|label|prediction|\n",
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|#quin69|2022-05-20T22:33:...|                  70|               raduk|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|       infernal cry?|          airfusionz|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|               24hr?|       lootumshootum|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|    level ur EA LOLW|             catatrr|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|                  72|          deathnell3|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|            mrbeannm|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|HIDEOUT SIMULATOR...|       culliganman76|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:33:...|             Weirdga|          adamoftime|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|                THIS|      chrono_crosser|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|           alejan2ro|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|                  69|   badly_coded_brain|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:33:...|     PauseChamp hodl|            mrbeannm|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|   THIS one is maxed|mikeauxsmoll_____...|  1.0|       1.0|\n",
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:33:53 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:53 WARN BlockManager: Block input-0-1653086033000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:54 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:54 WARN BlockManager: Block input-0-1653086034200 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:55 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:55 WARN BlockManager: Block input-0-1653086035000 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:33:56 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:33:56 WARN BlockManager: Block input-0-1653086036000 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:33:56 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:33:56 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/05/21 00:33:56 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:33:56 WARN ReceiverSupervisorImpl: Receiver has been stopped\n",
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:34:00 =========\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "|channel|            datetime|             message|        username|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "|#quin69|2022-05-20T22:33:...|                LOLW|  the_white_pepe|\n",
      "|#quin69|2022-05-20T22:33:...|                  11|     crypthicccc|\n",
      "| #lirik|2022-05-20T22:33:...|@NuIlzero Nice mo...|        xsalim69|\n",
      "|#quin69|2022-05-20T22:33:...|spec out of eleme...|          alex6x|\n",
      "| #lirik|2022-05-20T22:33:...| 3x PauseChamp combo|        fossabot|\n",
      "|#quin69|2022-05-20T22:33:...|monkaHeliWatchesW...|       sitkid721|\n",
      "|#quin69|2022-05-20T22:33:...|you just need to ...|verygoodplayerow|\n",
      "|#quin69|2022-05-20T22:33:...|Chat, does he has...|       metrandir|\n",
      "|#quin69|2022-05-20T22:33:...|duno but what abo...|       dkarkill1|\n",
      "|#quin69|2022-05-20T22:33:...|get veiled helmet...|        gynariel|\n",
      "| #lirik|2022-05-20T22:33:...|pepeFASTJAM pepeF...|  kirby_the_pink|\n",
      "| #lirik|2022-05-20T22:33:...|catJAM xar2EDM do...|           nrnee|\n",
      "| #lirik|2022-05-20T22:33:...|NomuJem xar2EDM  ...|          aur1so|\n",
      "| #lirik|2022-05-20T22:33:...|      MufasaPls djFR|          ninput|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|         sikefox|\n",
      "|#quin69|2022-05-20T22:33:...|20 hour crafting ...|    scorsunicorn|\n",
      "| #lirik|2022-05-20T22:33:...|              ratJAM|          crioos|\n",
      "| #lirik|2022-05-20T22:33:...|xar2EDM docPls xa...|          scluse|\n",
      "+-------+--------------------+--------------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:34:00 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/05/21 00:34:00 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n",
      "22/05/21 00:34:00 WARN DAGScheduler: Broadcasting large task binary with size 2.0 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "|channel|            datetime|             message|        username|label|prediction|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "|#quin69|2022-05-20T22:33:...|                LOLW|  the_white_pepe|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:33:...|                  11|     crypthicccc|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:33:...|@NuIlzero Nice mo...|        xsalim69|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|spec out of eleme...|          alex6x|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...| 3x PauseChamp combo|        fossabot|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|monkaHeliWatchesW...|       sitkid721|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:33:...|you just need to ...|verygoodplayerow|  1.0|       1.0|\n",
      "|#quin69|2022-05-20T22:33:...|Chat, does he has...|       metrandir|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|duno but what abo...|       dkarkill1|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|get veiled helmet...|        gynariel|  1.0|       1.0|\n",
      "| #lirik|2022-05-20T22:33:...|pepeFASTJAM pepeF...|  kirby_the_pink|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|catJAM xar2EDM do...|           nrnee|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|NomuJem xar2EDM  ...|          aur1so|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|      MufasaPls djFR|          ninput|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|          PauseChamp|         sikefox|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:33:...|20 hour crafting ...|    scorsunicorn|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|              ratJAM|          crioos|  0.0|       0.0|\n",
      "| #lirik|2022-05-20T22:33:...|xar2EDM docPls xa...|          scluse|  0.0|       0.0|\n",
      "+-------+--------------------+--------------------+----------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "globals()['models_loaded'] = False\n",
    "globals()['my_model'] = None\n",
    "\n",
    "def process(time, rdd):\n",
    "    if rdd.isEmpty():\n",
    "        return\n",
    "    \n",
    "    print(\"========= %s =========\" % str(time))\n",
    "    \n",
    "    # Convert to data frame\n",
    "    df = spark.read.json(rdd)\n",
    "    df.show()\n",
    "    \n",
    "    if not globals()['models_loaded']:\n",
    "        # load in your models here\n",
    "        globals()['my_model'] = PipelineModel.load(rfPath)\n",
    "        globals()['models_loaded'] = True\n",
    "        \n",
    "    # And then predict using the loaded model: \n",
    "    df_result = globals()['my_model'].transform(df)\n",
    "    df_result.select(\"channel\",\"datetime\",\"message\",\"username\",\"label\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext(sc, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ssc.socketTextStream(\"localhost\", 8080)\n",
    "lines.foreachRDD(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:34:58 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:34:58 WARN BlockManager: Block input-0-1653086098400 replicated to only 0 peer(s) instead of 1 peers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:35:00 =========\n",
      "+-------+--------------------+--------------------+-----------+\n",
      "|channel|            datetime|             message|   username|\n",
      "+-------+--------------------+--------------------+-----------+\n",
      "| #lirik|2022-05-20T22:34:...|xar2EDM docPls xa...|     scluse|\n",
      "|#quin69|2022-05-20T22:34:...|any sniffers? SNIFFA|    bragok2|\n",
      "|#quin69|2022-05-20T22:34:...|    champion or ele?|atheongames|\n",
      "+-------+--------------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:35:00 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:00 WARN BlockManager: Block input-0-1653086100400 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------+-----+----------+\n",
      "|channel|            datetime|             message|   username|label|prediction|\n",
      "+-------+--------------------+--------------------+-----------+-----+----------+\n",
      "| #lirik|2022-05-20T22:34:...|xar2EDM docPls xa...|     scluse|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:34:...|any sniffers? SNIFFA|    bragok2|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:34:...|    champion or ele?|atheongames|  1.0|       0.0|\n",
      "+-------+--------------------+--------------------+-----------+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:35:01 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:01 WARN BlockManager: Block input-0-1653086101400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:35:02 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:02 WARN BlockManager: Block input-0-1653086102400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:35:03 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:03 WARN BlockManager: Block input-0-1653086103400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:35:04 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:04 WARN BlockManager: Block input-0-1653086104400 replicated to only 0 peer(s) instead of 1 peers\n"
     ]
    }
   ],
   "source": [
    "ssc_t = StreamingThread(ssc)\n",
    "ssc_t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/21 00:35:05 WARN RandomBlockReplicationPolicy: Expecting 1 replicas with only 0 peer/s.\n",
      "22/05/21 00:35:05 WARN BlockManager: Block input-0-1653086105400 replicated to only 0 peer(s) instead of 1 peers\n",
      "22/05/21 00:35:05 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver\n",
      "22/05/21 00:35:05 WARN SocketReceiver: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:35:05 WARN ReceiverSupervisorImpl: Restarting receiver with delay 2000 ms: Error receiving data\n",
      "java.net.SocketException: Socket closed\n",
      "\tat java.base/java.net.SocketInputStream.socketRead0(Native Method)\n",
      "\tat java.base/java.net.SocketInputStream.socketRead(SocketInputStream.java:115)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:168)\n",
      "\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:284)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:326)\n",
      "\tat java.base/sun.nio.cs.StreamDecoder.read(StreamDecoder.java:178)\n",
      "\tat java.base/java.io.InputStreamReader.read(InputStreamReader.java:181)\n",
      "\tat java.base/java.io.BufferedReader.fill(BufferedReader.java:161)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:326)\n",
      "\tat java.base/java.io.BufferedReader.readLine(BufferedReader.java:392)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:121)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$2.getNext(SocketInputDStream.scala:119)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver.receive(SocketInputDStream.scala:91)\n",
      "\tat org.apache.spark.streaming.dstream.SocketReceiver$$anon$1.run(SocketInputDStream.scala:72)\n",
      "22/05/21 00:35:05 WARN ReceiverSupervisorImpl: Receiver has been stopped\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Stopping... this may take a few seconds -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"receiver-supervisor-future-0\" java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.base/java.lang.Thread.sleep(Native Method)\n",
      "\tat org.apache.spark.streaming.receiver.ReceiverSupervisor.$anonfun$restartReceiver$1(ReceiverSupervisor.scala:196)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:834)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========= 2022-05-21 00:35:10 =========\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|channel|            datetime|             message|            username|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "|#quin69|2022-05-20T22:34:...|               SNFFA|            polantis|\n",
      "|#quin69|2022-05-20T22:35:...|it's SNIFFA not S...|         hugejackman|\n",
      "| #lirik|2022-05-20T22:35:...|         MILKMANRAVE|              crioos|\n",
      "|#quin69|2022-05-20T22:35:...|              SNIFFA|flamethrowerfirin...|\n",
      "| #lirik|2022-05-20T22:35:...|@Crioos peepoHuggers|      kirby_the_pink|\n",
      "|#quin69|2022-05-20T22:35:...|             2Header|               prour|\n",
      "| #lirik|2022-05-20T22:35:...|         MILKMANRAVE|            nuilzero|\n",
      "|#quin69|2022-05-20T22:35:...|                  11|         crypthicccc|\n",
      "|#quin69|2022-05-20T22:35:...|             2Header|            manoekin|\n",
      "| #lirik|2022-05-20T22:35:...|lirikHMM I keep r...|           inate2052|\n",
      "|#quin69|2022-05-20T22:35:...|                WTFF|        lionkingler1|\n",
      "|#quin69|2022-05-20T22:35:...|why arent you say...|              arnday|\n",
      "| #lirik|2022-05-20T22:35:...|xar2EDM forsenPls...|              scluse|\n",
      "|#quin69|2022-05-20T22:35:...|You still have th...|        thecolours25|\n",
      "+-------+--------------------+--------------------+--------------------+\n",
      "\n",
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|channel|            datetime|             message|            username|label|prediction|\n",
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "|#quin69|2022-05-20T22:34:...|               SNFFA|            polantis|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|it's SNIFFA not S...|         hugejackman|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:35:...|         MILKMANRAVE|              crioos|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|              SNIFFA|flamethrowerfirin...|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:35:...|@Crioos peepoHuggers|      kirby_the_pink|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|             2Header|               prour|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:35:...|         MILKMANRAVE|            nuilzero|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|                  11|         crypthicccc|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|             2Header|            manoekin|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:35:...|lirikHMM I keep r...|           inate2052|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|                WTFF|        lionkingler1|  1.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|why arent you say...|              arnday|  1.0|       0.0|\n",
      "| #lirik|2022-05-20T22:35:...|xar2EDM forsenPls...|              scluse|  0.0|       0.0|\n",
      "|#quin69|2022-05-20T22:35:...|You still have th...|        thecolours25|  1.0|       0.0|\n",
      "+-------+--------------------+--------------------+--------------------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc_t.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "243px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
